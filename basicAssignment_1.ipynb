{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hindu-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for this notebook\n",
    "import gym\n",
    "import gym_gridworld\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "raw",
   "id": "widespread-myanmar",
   "metadata": {},
   "source": [
    "Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bizarre-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.5\n",
    "R = np.zeros((3,1))\n",
    "P = np.zeros((3,3))\n",
    "\n",
    "R[0] = -8 # For state A\n",
    "R[1] = 0 # For state B\n",
    "R[2] = 6 # For state C\n",
    "\n",
    "# Enter the probabilities going from state i to state j\n",
    "P[0] = [0, 1, 0] # for i=0 (state A)\n",
    "P[1] = [0.25, 0.25, 0.5] # for i=1 (state B)\n",
    "P[2] = [0.25, 0.75, 0] # for i=2 (state C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "chubby-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.82222222]\n",
      " [ 0.35555556]\n",
      " [ 5.15555556]]\n"
     ]
    }
   ],
   "source": [
    "V = np.linalg.inv(np.eye(3) - discount*P)@R # V = (I - discount*P)^-1 * R\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aboriginal-venue",
   "metadata": {},
   "source": [
    "Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "drawn-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    \n",
    "    def __init__(self, nA=4, nS=16):\n",
    "        self.nA = nA # Number of actions\n",
    "        self.nS = nS # Number of states\n",
    "        \n",
    "        # Uniform probabilites in each state.\n",
    "        # That is, in each of the nS states\n",
    "        # each of the nA actions has probability\n",
    "        # 1/nA.\n",
    "        self.probs = np.ones((nS,nA))/nA \n",
    "\n",
    "    def act(self, state, done):\n",
    "        action = np.random.choice(self.nA, p=self.probs[state]) \n",
    "        return action # a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "chronic-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_value(env, discount, s, a, v):\n",
    "    \n",
    "    action_value = 0\n",
    "    \n",
    "    for p, next_s, reward, _ in env.P[s][a]:\n",
    "        # Loop through all possible (s', r) pairs\n",
    "        action_value += p*(reward + (discount * v[next_s]))\n",
    "    \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "conscious-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n",
    "    \n",
    "    v = v0\n",
    "    \n",
    "    for i in range(max_iter): # Loop\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            vs = v[s]\n",
    "            \n",
    "            # Code for updating v[s]\n",
    "            action_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                # Compute action value for all actions\n",
    "                action_values[a] = (env, discount, s, a, v)\n",
    "                \n",
    "            v[s] = np.max(action_values)\n",
    "            \n",
    "            delta = np.max([delta, np.abs(vs-v[s])])\n",
    "            \n",
    "        if (delta < tol): # Until delta < tol\n",
    "            break\n",
    "            \n",
    "    return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "entertaining-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(env, discount, agent, v):\n",
    "    \n",
    "    # The new policy will be a_probs\n",
    "    # We start by setting all probabilities to 0\n",
    "    # Then when we have found the greedy action in a state, \n",
    "    # we change the probability for that action to 1.0.\n",
    "    \n",
    "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for a in range(env.action_space.n):\n",
    "            # Compute action value for all actions\n",
    "            action_values[a] = compute_action_value(env, discount, s, a, v)\n",
    "            \n",
    "        a_max = np.argmax(action_values) # A greedy action\n",
    "        a_probs[s][a_max] = 1.0 # Always choose the greedy action!\n",
    "        \n",
    "    return a_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "twelve-samuel",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'tuple'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-70c681a13341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-cdaea4f9a009>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(env, discount, agent, v0, max_iter, tol)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;31m# Compute action value for all actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0maction_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "\n",
    "v0 = np.zeros(env.observation_space.n)\n",
    "v = value_iteration(env, discount, agent, v0)\n",
    "print(v.reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "presidential-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.probs = greedy_policy(env, discount, agent, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "pressing-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9264147800954421\n",
      "[0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(v[18])\n",
    "print(agent.probs[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-wireless",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
