{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym_gridworld\n",
    "import gymgrid\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(agent, env, n_episodes):\n",
    "    step = 0\n",
    "    steps = np.zeros(n_episodes) # Steps after each episode\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        states = [state]\n",
    "        action = agent.act(state)\n",
    "        actions = [action]\n",
    "        rewards = [0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        T = np.inf\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "                states.append(state_next)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                if done == True:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    action_next = agent.act(state_next)\n",
    "                    actions.append(action_next)\n",
    "            tau = t - agent.n + 1\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + agent.n + 1, T + 1)):\n",
    "                    G += np.power(agent.gamma, i - tau- 1) * rewards[i]\n",
    "                if (tau + agent.n) < T:\n",
    "                    G += np.power(agent.gamma, agent.n) * agent.Q[states[tau + agent.n], actions[tau + agent.n]]  \n",
    "                agent.learn(states[tau], actions[tau], G)\n",
    "                action = action_next\n",
    "                step += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "            t += 1\n",
    "        \n",
    "        steps[episode] = step\n",
    "        total_rewards[episode] = episode_reward\n",
    "        \n",
    "    return total_rewards, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA():\n",
    "    def __init__(self, n_states, n_actions, gamma, alpha, epsilon, n=1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        self.n = n\n",
    "        \n",
    "    def act(self, state):\n",
    "        # You can use np.random.choice(self.n_actions) to get a random action\n",
    "        # Implement epsilon-greedy policy\n",
    "        choices = [np.argmax(self.Q[state, :]), np.random.choice(self.n_actions)]\n",
    "        action = np.random.choice(choices, 1, p=[1 - self.epsilon, self.epsilon])\n",
    "        return action[0]\n",
    "            \n",
    "    def learn(self, s, a, G):\n",
    "        # Implement the TD(0) update of Q (see equation (6.7) in textbook)\n",
    "        self.Q[s, a] = self.Q[s, a] + self.alpha * (G - self.Q[s,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "episodes = 1000\n",
    "n_values = [1, 2, 5, 10, 50]\n",
    "train_repetition = 20\n",
    "concat_mean_rewards = np.zeros((len(n_values), episodes))\n",
    "\n",
    "for counter, n_value in enumerate(n_values):\n",
    "    list_rewards = np.zeros((train_repetition, episodes))\n",
    "    for training in range(0, train_repetition):\n",
    "        agent = SARSA(env.observation_space.n, env.action_space.n, gamma=1.0, alpha=0.1, epsilon=0.1, n=n_value)\n",
    "        total_rewards, _ = train_sarsa(agent, env, n_episodes=episodes)\n",
    "        list_rewards[training,:] = total_rewards\n",
    "    mean_rewards = np.mean(list_rewards, axis=0) # Compute mean column-wise\n",
    "    concat_mean_rewards[counter,:] = mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter, n_value in enumerate(n_values):\n",
    "    plt.plot(range(1, episodes+1), concat_mean_rewards[counter,:])\n",
    "    plt.title(f'n = {n_value}')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average total reward')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_episodes = 10000\n",
    "list_rewards = np.zeros((train_repetition, new_episodes))\n",
    "for training in range(0, train_repetition):\n",
    "    agent = SARSA(env.observation_space.n, env.action_space.n, gamma=1.0, alpha=0.1, epsilon=0.1, n=50)\n",
    "    total_rewards, _ = train_sarsa(agent, env, n_episodes=new_episodes)\n",
    "    list_rewards[training,:] = total_rewards\n",
    "mean_rewards = np.mean(list_rewards, axis=0)\n",
    "\n",
    "plt.plot(range(1, new_episodes+1), mean_rewards)\n",
    "plt.title(f'n = 50')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average total reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
